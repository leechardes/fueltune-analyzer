"""
Comprehensive unit tests for data parsing modules.

Tests CSV parser, data validators, normalizers, and related
data processing functionality with extensive coverage.
"""

import csv
import tempfile
from pathlib import Path

import numpy as np
import pandas as pd
import pytest

from src.data.csv_parser import CSVParser, CSVParsingError
from src.data.normalizer import DataNormalizer
from src.data.quality import DataQualityAssessor as DataQualityChecker
from src.data.validators import DataValidator


class TestDataParser:
    """Test comprehensive data parsing functionality."""

    def test_csv_parser_initialization(self):
        """Test CSV parser initialization."""
        parser = CSVParser()
        assert isinstance(parser, CSVParser)
        assert hasattr(parser, "FIELD_MAPPINGS_37")
        assert hasattr(parser, "FIELD_MAPPINGS_64")

    def test_parse_realistic_fueltech_data(self, realistic_telemetry_data):
        """Test parsing realistic telemetry data."""
        parser = CSVParser()

        # Create temporary CSV with realistic data
        with tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False) as f:
            realistic_telemetry_data.to_csv(f.name, index=False)
            csv_path = Path(f.name)

        try:
            result = parser.parse(csv_path)
            assert isinstance(result, pd.DataFrame)
            assert len(result) == len(realistic_telemetry_data)
            assert not result.empty

            # Verify data integrity
            assert "rpm" in result.columns or "RPM" in result.columns
            assert "throttle" in result.columns or "TPS" in result.columns

        finally:
            csv_path.unlink(missing_ok=True)

    def test_parse_fueltech_37_format(self, fueltech_37_field_data):
        """Test parsing FuelTech 37-field format."""
        parser = CSVParser()

        with tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False) as f:
            fueltech_37_field_data.to_csv(f.name, index=False)
            csv_path = Path(f.name)

        try:
            result = parser.parse(csv_path)
            assert isinstance(result, pd.DataFrame)
            assert len(result) == len(fueltech_37_field_data)

            # Check for expected FuelTech fields
            expected_fields = ["TIME", "RPM", "TPS"]
            for field in expected_fields:
                assert field in result.columns or field.lower() in result.columns

        finally:
            csv_path.unlink(missing_ok=True)

    def test_format_detection(self):
        """Test automatic format detection."""
        parser = CSVParser()

        # Create 37-field CSV
        headers_37 = [f"Field_{i}" for i in range(37)]
        data_37 = [[i for i in range(37)] for _ in range(3)]

        with tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False) as f:
            writer = csv.writer(f)
            writer.writerow(headers_37)
            writer.writerows(data_37)
            csv_path_37 = Path(f.name)

        try:
            format_37 = parser.detect_format(csv_path_37)
            assert format_37 is not None
            assert isinstance(format_37, str)

        finally:
            csv_path_37.unlink(missing_ok=True)

        # Create 64-field CSV
        headers_64 = [f"Field_{i}" for i in range(64)]
        data_64 = [[i for i in range(64)] for _ in range(3)]

        with tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False) as f:
            writer = csv.writer(f)
            writer.writerow(headers_64)
            writer.writerows(data_64)
            csv_path_64 = Path(f.name)

        try:
            format_64 = parser.detect_format(csv_path_64)
            assert format_64 is not None
            assert isinstance(format_64, str)
            assert format_64 != format_37  # Different formats should be detected differently

        finally:
            csv_path_64.unlink(missing_ok=True)

    def test_field_mapping_functionality(self, field_mapping_test_data):
        """Test field mapping and normalization."""
        parser = CSVParser()

        # Test field mappings exist
        assert isinstance(parser.FIELD_MAPPINGS_37, dict)
        assert len(parser.FIELD_MAPPINGS_37) > 0

        # Test specific mappings
        original_fields = field_mapping_test_data["fueltech_37_original"]
        for field in original_fields:
            if field in parser.FIELD_MAPPINGS_37:
                mapped_field = parser.FIELD_MAPPINGS_37[field]
                assert isinstance(mapped_field, str)
                assert len(mapped_field) > 0

    def test_error_handling(self, corrupt_csv_data):
        """Test error handling for various corrupt data types."""
        parser = CSVParser()

        for corruption_type, corrupt_content in corrupt_csv_data.items():
            with tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False) as f:
                f.write(corrupt_content)
                csv_path = Path(f.name)

            try:
                # Should handle corruption gracefully
                with pytest.raises(
                    (
                        CSVParsingError,
                        pd.errors.EmptyDataError,
                        pd.errors.ParserError,
                        ValueError,
                        FileNotFoundError,
                    )
                ):
                    parser.parse(csv_path)

            finally:
                csv_path.unlink(missing_ok=True)

    def test_large_file_handling(self, performance_test_data):
        """Test handling of large files."""
        parser = CSVParser()

        with tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False) as f:
            performance_test_data.to_csv(f.name, index=False)
            csv_path = Path(f.name)

        try:
            import time

            start_time = time.time()
            result = parser.parse(csv_path)
            parse_time = time.time() - start_time

            assert isinstance(result, pd.DataFrame)
            assert len(result) == len(performance_test_data)
            # Should parse large files in reasonable time
            assert parse_time < 60.0  # Less than 1 minute for 50k rows

        finally:
            csv_path.unlink(missing_ok=True)

    def test_edge_cases(self, empty_dataframe, single_row_data):
        """Test edge cases like empty and single-row data."""
        parser = CSVParser()

        # Test empty DataFrame validation
        validation_result = parser.validate(empty_dataframe)
        assert isinstance(validation_result, dict)
        assert "is_valid" in validation_result

        # Test single-row DataFrame validation
        validation_result = parser.validate(single_row_data)
        assert isinstance(validation_result, dict)
        assert "is_valid" in validation_result
        assert validation_result.get("row_count", 0) == 1

    def test_time_series_handling(self, time_series_gaps_data):
        """Test handling of time series data with gaps."""
        parser = CSVParser()

        # Create CSV with time gaps
        with tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False) as f:
            time_series_gaps_data.to_csv(f.name, index=False)
            csv_path = Path(f.name)

        try:
            result = parser.parse(csv_path)
            assert isinstance(result, pd.DataFrame)
            assert len(result) == len(time_series_gaps_data)

            # Verify timestamp column exists
            timestamp_cols = [
                col for col in result.columns if "time" in col.lower() or "timestamp" in col.lower()
            ]
            assert len(timestamp_cols) > 0

        finally:
            csv_path.unlink(missing_ok=True)


class TestDataValidator:
    """Test data validation functionality."""

    def test_validator_initialization(self):
        """Test data validator initialization."""
        try:
            validator = DataValidator()
            assert isinstance(validator, DataValidator)
        except ImportError:
            # Module might not exist yet, skip test
            pytest.skip("DataValidator not implemented")

    def test_validation_rules(self, realistic_telemetry_data):
        """Test basic validation rules."""
        try:
            validator = DataValidator()

            # Test numeric range validation
            rpm_values = realistic_telemetry_data["rpm"]
            valid_rpm = validator.validate_numeric_range(rpm_values, min_value=0, max_value=10000)
            assert isinstance(valid_rpm, (bool, dict, list))

        except (ImportError, AttributeError):
            pytest.skip("DataValidator validation methods not implemented")

    def test_anomaly_detection_integration(self, anomaly_data):
        """Test integration with anomaly detection."""
        try:
            validator = DataValidator()

            # Test if validator can detect known anomalies
            validation_result = validator.validate(anomaly_data)
            assert isinstance(validation_result, dict)

            # Check if anomalies were flagged
            if "anomalies" in validation_result:
                assert len(validation_result["anomalies"]) > 0

        except (ImportError, AttributeError):
            pytest.skip("DataValidator anomaly detection not implemented")

    def test_extreme_values_validation(self, extreme_values_data):
        """Test validation of extreme but valid values."""
        try:
            validator = DataValidator()

            validation_result = validator.validate(extreme_values_data)
            assert isinstance(validation_result, dict)
            assert "is_valid" in validation_result or "errors" in validation_result

        except (ImportError, AttributeError):
            pytest.skip("DataValidator not fully implemented")


class TestDataNormalizer:
    """Test data normalization functionality."""

    def test_normalizer_initialization(self):
        """Test data normalizer initialization."""
        try:
            normalizer = DataNormalizer()
            assert isinstance(normalizer, DataNormalizer)
        except ImportError:
            pytest.skip("DataNormalizer not implemented")

    def test_field_name_normalization(self, fueltech_37_field_data):
        """Test field name normalization."""
        try:
            normalizer = DataNormalizer()

            original_columns = list(fueltech_37_field_data.columns)
            normalized_data = normalizer.normalize(fueltech_37_field_data)

            assert isinstance(normalized_data, pd.DataFrame)
            assert len(normalized_data) == len(fueltech_37_field_data)

            # Check that normalization occurred
            normalized_columns = list(normalized_data.columns)
            assert normalized_columns != original_columns

        except (ImportError, AttributeError):
            pytest.skip("DataNormalizer not fully implemented")

    def test_data_type_normalization(self, realistic_telemetry_data):
        """Test data type normalization."""
        try:
            normalizer = DataNormalizer()

            # Convert some columns to strings to test type normalization
            test_data = realistic_telemetry_data.copy()
            test_data["rpm"] = test_data["rpm"].astype(str)
            test_data["throttle"] = test_data["throttle"].astype(str)

            normalized_data = normalizer.normalize(test_data)

            # Check that numeric columns were converted back
            assert pd.api.types.is_numeric_dtype(normalized_data["rpm"])
            assert pd.api.types.is_numeric_dtype(normalized_data["throttle"])

        except (ImportError, AttributeError):
            pytest.skip("DataNormalizer type conversion not implemented")

    def test_unit_conversion(self):
        """Test unit conversion functionality."""
        try:
            normalizer = DataNormalizer()

            # Test temperature conversion (if implemented)
            celsius_temps = pd.Series([0, 25, 100])
            if hasattr(normalizer, "celsius_to_fahrenheit"):
                fahrenheit_temps = normalizer.celsius_to_fahrenheit(celsius_temps)
                assert fahrenheit_temps.iloc[0] == 32  # 0째C = 32째F
                assert fahrenheit_temps.iloc[2] == 212  # 100째C = 212째F

        except (ImportError, AttributeError):
            pytest.skip("Unit conversion not implemented")


class TestDataQualityChecker:
    """Test data quality checking functionality."""

    def test_quality_checker_initialization(self):
        """Test data quality checker initialization."""
        try:
            checker = DataQualityChecker()
            assert isinstance(checker, DataQualityChecker)
        except ImportError:
            pytest.skip("DataQualityChecker not implemented")

    def test_missing_data_detection(self):
        """Test missing data detection."""
        try:
            checker = DataQualityChecker()

            # Create data with missing values
            data_with_missing = pd.DataFrame(
                {
                    "rpm": [1000, np.nan, 2000, 2500],
                    "throttle": [10.0, 20.0, np.nan, 40.0],
                    "boost": [1.0, 1.5, 2.0, 2.5],
                }
            )

            quality_report = checker.check_quality(data_with_missing)
            assert isinstance(quality_report, dict)

            # Should detect missing values
            if "missing_values" in quality_report:
                assert quality_report["missing_values"]["rpm"] == 1
                assert quality_report["missing_values"]["throttle"] == 1
                assert quality_report["missing_values"]["boost"] == 0

        except (ImportError, AttributeError):
            pytest.skip("DataQualityChecker missing data detection not implemented")

    def test_outlier_detection(self, anomaly_data):
        """Test outlier detection functionality."""
        try:
            checker = DataQualityChecker()

            quality_report = checker.check_quality(anomaly_data)
            assert isinstance(quality_report, dict)

            # Should detect outliers in anomaly data
            if "outliers" in quality_report:
                assert len(quality_report["outliers"]) > 0

        except (ImportError, AttributeError):
            pytest.skip("DataQualityChecker outlier detection not implemented")

    def test_data_completeness(self, realistic_telemetry_data):
        """Test data completeness assessment."""
        try:
            checker = DataQualityChecker()

            quality_report = checker.check_quality(realistic_telemetry_data)
            assert isinstance(quality_report, dict)

            # Should assess completeness
            if "completeness" in quality_report:
                completeness = quality_report["completeness"]
                assert 0 <= completeness <= 1  # Should be a percentage/ratio

        except (ImportError, AttributeError):
            pytest.skip("DataQualityChecker completeness check not implemented")

    def test_data_consistency(self, time_series_gaps_data):
        """Test data consistency checks."""
        try:
            checker = DataQualityChecker()

            quality_report = checker.check_quality(time_series_gaps_data)
            assert isinstance(quality_report, dict)

            # Should detect time gaps as consistency issues
            if "consistency_issues" in quality_report:
                issues = quality_report["consistency_issues"]
                assert isinstance(issues, (list, dict))

        except (ImportError, AttributeError):
            pytest.skip("DataQualityChecker consistency check not implemented")


class TestIntegrationParsing:
    """Integration tests for parsing components working together."""

    def test_full_parsing_pipeline(self, realistic_telemetry_data):
        """Test complete parsing pipeline from CSV to clean data."""
        # Create temporary CSV
        with tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False) as f:
            realistic_telemetry_data.to_csv(f.name, index=False)
            csv_path = Path(f.name)

        try:
            # Step 1: Parse CSV
            parser = CSVParser()
            parsed_data = parser.parse(csv_path)
            assert isinstance(parsed_data, pd.DataFrame)

            # Step 2: Validate data (if validator exists)
            try:
                validator = DataValidator()
                validation_result = validator.validate(parsed_data)
                assert isinstance(validation_result, dict)
            except ImportError:
                pass

            # Step 3: Normalize data (if normalizer exists)
            try:
                normalizer = DataNormalizer()
                normalized_data = normalizer.normalize(parsed_data)
                assert isinstance(normalized_data, pd.DataFrame)
            except ImportError:
                normalized_data = parsed_data

            # Step 4: Quality check (if quality checker exists)
            try:
                checker = DataQualityChecker()
                quality_report = checker.check_quality(normalized_data)
                assert isinstance(quality_report, dict)
            except ImportError:
                pass

            # Final result should be a valid DataFrame
            assert isinstance(normalized_data, pd.DataFrame)
            assert len(normalized_data) > 0

        finally:
            csv_path.unlink(missing_ok=True)

    def test_error_propagation(self, corrupt_csv_data):
        """Test how errors propagate through the parsing pipeline."""
        for corruption_type, corrupt_content in corrupt_csv_data.items():
            with tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False) as f:
                f.write(corrupt_content)
                csv_path = Path(f.name)

            try:
                parser = CSVParser()

                # Should handle errors gracefully at each step
                with pytest.raises(
                    (
                        CSVParsingError,
                        pd.errors.EmptyDataError,
                        pd.errors.ParserError,
                        ValueError,
                        FileNotFoundError,
                    )
                ):
                    parsed_data = parser.parse(csv_path)

                    # If parsing succeeds with corrupt data, validate should catch issues
                    if parsed_data is not None:
                        try:
                            validator = DataValidator()
                            validation_result = validator.validate(parsed_data)
                            # Validation should detect issues
                            assert not validation_result.get("is_valid", True)
                        except ImportError:
                            pass

            finally:
                csv_path.unlink(missing_ok=True)

    def test_performance_pipeline(self, performance_test_data):
        """Test performance of complete parsing pipeline."""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False) as f:
            performance_test_data.to_csv(f.name, index=False)
            csv_path = Path(f.name)

        try:
            import time

            start_time = time.time()

            # Run complete pipeline
            parser = CSVParser()
            parsed_data = parser.parse(csv_path)

            try:
                normalizer = DataNormalizer()
                normalized_data = normalizer.normalize(parsed_data)
            except ImportError:
                normalized_data = parsed_data

            try:
                checker = DataQualityChecker()
                checker.check_quality(normalized_data)
            except ImportError:
                pass

            total_time = time.time() - start_time

            # Pipeline should complete in reasonable time
            assert total_time < 120.0  # Less than 2 minutes for 50k rows
            assert isinstance(normalized_data, pd.DataFrame)
            assert len(normalized_data) == len(performance_test_data)

        finally:
            csv_path.unlink(missing_ok=True)
